{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be66d93d-62e8-4ccf-8521-cad49a9f95b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "import requests\n",
    "import pandas\n",
    "import json\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a08313-6d2b-4b98-9e71-80720232ac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19723a99-4125-45ca-a497-699cda7f8d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_hits = []\n",
    "with open('./hits_unique.json') as file:\n",
    "    for line in file:\n",
    "        unique_hits.append(json.loads(line))\n",
    "\n",
    "unique_historic_hits = []\n",
    "with open('./hits_unique_historic.json') as file:\n",
    "    for line in file:\n",
    "        unique_historic_hits.append(json.loads(line))\n",
    "\n",
    "non_unique_hits = []\n",
    "with open('./hits.json') as file:\n",
    "    for line in file:\n",
    "        non_unique_hits.append(json.loads(line))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55df77bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## HELPER METHODS\n",
    "\n",
    "def get_number_of_dblp_records(orcid: str):\n",
    "    r = requests.get(f'http://dblp2.uni-trier.de/orcid/{orcid}.xml?view=ajax')\n",
    "    if (r.status_code == 404):\n",
    "        return 0\n",
    "    if (not r.ok):\n",
    "        raise Exception(f\"{str} got an {r.status_code}\")\n",
    "    z = re.search(r'n=\"(\\d*)\"', r.text)\n",
    "    records_in_dblp = int(z.group(1))\n",
    "    time.sleep(0.1)\n",
    "    return records_in_dblp\n",
    "\n",
    "\n",
    "def get_dblp_pid_by_orcid(orcid: str):\n",
    "    r = requests.get(f'http://dblp2.uni-trier.de:8000/?q=:facetid:eid:\"ORCID:{orcid}\"&format=json&p=1')\n",
    "    if (r.status_code == 404):\n",
    "        return None\n",
    "    if (not r.ok):\n",
    "        raise Exception(f\"{str} got an {r.status_code}\")\n",
    "    record = r.json()['result']['hits']['hit'][0]['info']['record']\n",
    "    z = re.search(fr'<(?:author|editor) pid=\"([^\"]*)\" orcid=\"{orcid}\">', record)\n",
    "    if(z is None):\n",
    "        print(r.text)\n",
    "        raise Exception()\n",
    "    pid = z.group(1)\n",
    "    return pid, None\n",
    "\n",
    "\n",
    "def get_dblp_record_count_by_pid(pid: str):\n",
    "    r = requests.get(f\"https://dblp2.uni-trier.de/pid/{pid}.xml?view=ajax\")\n",
    "    if (r.status_code == 404):\n",
    "        return None\n",
    "    if (not r.ok):\n",
    "        raise Exception(f\"{str} got an {r.status_code}\")\n",
    "    z = re.search(r'n=\"(\\d*)\"', r.text)\n",
    "    if(z is None):\n",
    "        print(r.text)\n",
    "        raise Exception()\n",
    "    records_in_dblp = int(z.group(1))\n",
    "    return records_in_dblp\n",
    "\n",
    "\n",
    "def get_number_of_dblp_records_extended(orcid: str):\n",
    "    pid = get_dblp_pid_by_orcid(orcid)\n",
    "    if(pid is None):\n",
    "        return 0\n",
    "\n",
    "    records_in_dblp = get_dblp_record_count_by_pid(pid)\n",
    "    if (records_in_dblp is None):\n",
    "        return 0\n",
    "        \n",
    "    time.sleep(0.1)\n",
    "    return records_in_dblp\n",
    "\n",
    "\n",
    "def get_versions_conceptrecid(recid):\n",
    "    BASE_URL = \"https://zenodo.org\"\n",
    "\n",
    "    params = {\n",
    "        'q': f'conceptrecid:{recid}',\n",
    "        'all_versions': 'true',\n",
    "        'exact': 'true',\n",
    "        'size': 1000\n",
    "    }\n",
    "\n",
    "    r = requests.get(BASE_URL + \"/api/records/\", params=params)\n",
    "\n",
    "    remaining = int(r.headers['X-RateLimit-Remaining'])\n",
    "    reset_time_utc = datetime.fromtimestamp(int(r.headers['X-RateLimit-Reset']), timezone.utc)\n",
    "    print(reset_time_utc)\n",
    "\n",
    "    if remaining == 0:\n",
    "        current_time = datetime.now(timezone.utc)\n",
    "        print(reset_time_utc)\n",
    "        print(current_time)\n",
    "        time_to_wait_in_seconds = (reset_time_utc - current_time).total_seconds()\n",
    "        print(2)\n",
    "        print(f\"Waiting for {time_to_wait_in_seconds} seconds\")\n",
    "        time.sleep(time_to_wait_in_seconds)\n",
    "\n",
    "    df = r.json()\n",
    "    concept_records = pandas.json_normalize(df['hits']['hits'])\n",
    "\n",
    "    return concept_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003c1ae6-6108-4431-9a4b-d5f723d0b354",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PREPARE DATASETS\n",
    "unique_records = pandas.json_normalize(unique_hits)\n",
    "unique_historic_records = pandas.json_normalize(unique_historic_hits)\n",
    "nonunique_records = pandas.json_normalize(non_unique_hits)\n",
    "\n",
    "\n",
    "dblp_orcids = pandas.read_fwf('./unique_orcids.txt')\n",
    "unique_orcid = pandas.read_pickle('./unique_orcids_with_dblp_record_counts.pickle')\n",
    "versions_of_concepts = pandas.read_pickle('./concept_versions.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14786f7e-6a01-4a4a-9001-35c7ef8cefe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PREPARE METRICS\n",
    "\n",
    "# Q: Count of disctinc conceptdoi\n",
    "\n",
    "count_unique_concept_doi = unique_records['conceptdoi'].nunique()\n",
    "\n",
    "## Q: Count number of authors per dataset, get min, max, avg, median\n",
    "unique_records['#creators'] = unique_records['metadata.creators'].apply(lambda x: len(x))\n",
    "\n",
    "creators_count_max = unique_records['#creators'].max()\n",
    "creators_count_min = unique_records['#creators'].min()\n",
    "creators_count_median = unique_records['#creators'].median()\n",
    "creators_count_mean = unique_records['#creators'].mean()\n",
    "creators_count_quantile95 = unique_records['#creators'].quantile(q=0.95)\n",
    "\n",
    "## Versions\n",
    "unique_records['#versions'] = unique_records['metadata.relations.version'].apply(lambda x: x[0]['count'])\n",
    "\n",
    "version_count_max = unique_records['#versions'].max()\n",
    "version_count_min = unique_records['#versions'].min()\n",
    "version_count_median = unique_records['#versions'].median()\n",
    "version_count_mean = unique_records['#versions'].mean()\n",
    "version_count_quantile95 = unique_records['#versions'].quantile(q=0.95)\n",
    "version_count_exactly_one_version = len(unique_records[unique_records['#versions'] == 1])\n",
    "\n",
    "\n",
    "## Historic Verisons\n",
    "unique_historic_records['#versions'] = unique_historic_records['metadata.relations.version'].apply(lambda x: x[0]['count'])\n",
    "\n",
    "version_historic_count_max = unique_historic_records['#versions'].max()\n",
    "version_historic_count_min = unique_historic_records['#versions'].min()\n",
    "version_historic_count_median = unique_historic_records['#versions'].median()\n",
    "version_historic_count_mean = unique_historic_records['#versions'].mean()\n",
    "version_historic_count_quantile95 = unique_historic_records['#versions'].quantile(q=0.95)\n",
    "\n",
    "# Question: do the authors change between versions?\n",
    "multiple_versions = nonunique_records.groupby('conceptdoi')['doi'].size()\n",
    "multiple_versions = multiple_versions[multiple_versions > 1]\n",
    "records_with_multiple_versions = pandas.merge(nonunique_records, multiple_versions, how='inner', on='conceptdoi')\n",
    "records_with_multiple_versions['#creators'] = records_with_multiple_versions['metadata.creators'].apply(lambda x: len(x))\n",
    "changing_creator_count = records_with_multiple_versions[['conceptdoi', '#creators']].groupby('conceptdoi').nunique()\n",
    "\n",
    "multiple_versions_concept_doi_count = records_with_multiple_versions['conceptdoi'].nunique()\n",
    "multiple_versions_changing_authors_count = changing_creator_count[changing_creator_count['#creators'] > 1].size\n",
    "\n",
    "## Communities\n",
    "communmities = unique_records[['metadata.communities']].explode('metadata.communities')\n",
    "communmities_clean = communmities[communmities['metadata.communities'].notna()]\n",
    "communmities_clean['metadata.communities'] = communmities_clean['metadata.communities'].apply(lambda x: x['id'])\n",
    "communmities_agg = communmities_clean.groupby('metadata.communities').agg(count=('metadata.communities', 'count')).sort_values(by=['count'], ascending=False)\n",
    "\n",
    "top_20_communities = list(communmities_agg.head(20).itertuples(index=True, name=None))\n",
    "communities_quantile = communmities_agg['count'].quantile(q=0.95)\n",
    "communities_mean = communmities_agg['count'].mean()\n",
    "communities_median = communmities_agg['count'].median()\n",
    "communities_max = communmities_agg['count'].max()\n",
    "communities_min = communmities_agg['count'].min()\n",
    "\n",
    "\n",
    "# DBLP authors\n",
    "doi_signatures_historic = unique_historic_records[['doi', 'metadata.creators']].explode('metadata.creators')\n",
    "doi_signatures_historic['affiliation'] = doi_signatures_historic['metadata.creators'].apply(lambda x: x.get('affiliation', None))\n",
    "doi_signatures_historic['name'] = doi_signatures_historic['metadata.creators'].apply(lambda x: x.get('name', None))\n",
    "doi_signatures_historic['orcid'] = doi_signatures_historic['metadata.creators'].apply(lambda x: x.get('orcid', None))\n",
    "\n",
    "records_historic_with_orcid_count = doi_signatures_historic.groupby('doi').agg({\"orcid\": pandas.Series.nunique}).sort_values(by='orcid')\n",
    "records_historic_dblp_authors = doi_signatures_historic[doi_signatures_historic.orcid.isin(dblp_orcids.orcid)]\n",
    "\n",
    "count_signatures_hisotric = len(doi_signatures_historic)\n",
    "count_signatures_with_orcid_hisotric = len(doi_signatures_historic.dropna(subset=['orcid']))\n",
    "count_records_with_at_least_one_orcid_hisotric = len(records_historic_with_orcid_count[records_historic_with_orcid_count.orcid > 0])\n",
    "count_records_with_a_dblp_match_hisotric = records_historic_dblp_authors['doi'].nunique()\n",
    "count_records_with_a_verified_dblp_match_hisotric = doi_signatures_historic[doi_signatures_historic.orcid.isin(unique_orcid[unique_orcid.number_of_rdblp_recods > 0]['orcid'])]['doi'].nunique()\n",
    "\n",
    "## DBLP authors historic \n",
    "doi_signatures = unique_records[['doi', 'metadata.creators']].explode('metadata.creators')\n",
    "doi_signatures['affiliation'] = doi_signatures['metadata.creators'].apply(lambda x: x.get('affiliation', None))\n",
    "doi_signatures['name'] = doi_signatures['metadata.creators'].apply(lambda x: x.get('name', None))\n",
    "doi_signatures['orcid'] = doi_signatures['metadata.creators'].apply(lambda x: x.get('orcid', None))\n",
    "\n",
    "records_with_orcid_count = doi_signatures.groupby('doi').agg({\"orcid\": pandas.Series.nunique}).sort_values(by='orcid')\n",
    "records_dblp_authors = doi_signatures[doi_signatures.orcid.isin(dblp_orcids.orcid)]\n",
    "\n",
    "count_signatures = len(doi_signatures)\n",
    "count_signatures_with_orcid = len(doi_signatures.dropna(subset=['orcid']))\n",
    "count_records_with_at_least_one_orcid = len(records_with_orcid_count[records_with_orcid_count.orcid > 0])\n",
    "count_records_with_a_dblp_match = records_dblp_authors['doi'].nunique()\n",
    "count_records_with_a_verified_dblp_match = doi_signatures[doi_signatures.orcid.isin(unique_orcid[unique_orcid.number_of_rdblp_recods > 0]['orcid'])]['doi'].nunique()\n",
    "\n",
    "\n",
    "\n",
    "# unique_orcid = records_dblp_authors.groupby('orcid').size().reset_index(level=0)\n",
    "# unique_orcid['number_of_rdblp_recods'] = unique_orcid['orcid'].apply(lambda x: get_number_of_dblp_records(x))\n",
    "# unique_orcid['number_of_rdblp_recods_extended'] = unique_orcid['orcid'].apply(lambda x: get_number_of_dblp_records_extended(x))\n",
    "\n",
    "unique_orcid = pandas.read_pickle('./unique_orcids_with_dblp_record_counts.pickle')\n",
    "\n",
    "count_verified_dblp_orcid = len(unique_orcid[unique_orcid.number_of_rdblp_recods > 0])\n",
    "\n",
    "\n",
    "## Version Updates\n",
    "\n",
    "# versions_of_concepts = pandas.DataFrame()\n",
    "# unique_several_versions = unique_historic_records[unique_historic_records['#versions'] > 1]\n",
    "# for recid in unique_several_versions['conceptrecid']:\n",
    "#     df_one = get_versions_conceptrecid(recid)\n",
    "#     versions_of_concepts = pandas.concat([versions_of_concepts, df_one], ignore_index=True)\n",
    "# versions_of_concepts.to_pickle('./concept_versions.pickle')\n",
    "\n",
    "versions_of_concepts['created'] = pandas.to_datetime(versions_of_concepts['created'])\n",
    "versions_of_concepts = versions_of_concepts.sort_values(by=['created'], ascending=True)\n",
    "versions_of_concepts['prev'] = versions_of_concepts.groupby('conceptrecid')['created'].shift()\n",
    "\n",
    "versions_of_concepts['time_delta'] = (versions_of_concepts['created'] - versions_of_concepts['prev']) / pandas.Timedelta(days=1)\n",
    "concepts_update_time = versions_of_concepts.groupby('conceptrecid').agg(avg_time_delta=('time_delta', 'mean'), count=('time_delta', 'count'))\n",
    "\n",
    "\n",
    "concept_min_update_time = float(concepts_update_time['avg_time_delta'].min())\n",
    "concept_max_update_time = float(concepts_update_time['avg_time_delta'].max())\n",
    "concept_mean_update_time = float(concepts_update_time['avg_time_delta'].mean())\n",
    "concept_median_update_time = float(concepts_update_time['avg_time_delta'].median())\n",
    "concept_quantile = float(concepts_update_time['avg_time_delta'].quantile(0.2))\n",
    "\n",
    "## Export of verified datasets\n",
    "\n",
    "unique_doi_verified = doi_signatures[doi_signatures.orcid.isin(unique_orcid[unique_orcid.number_of_rdblp_recods > 0]['orcid'])]['doi'].unique()\n",
    "verified_datasets = unique_records[unique_records['doi'].isin(unique_doi_verified)]\n",
    "output_verified = verified_datasets[['conceptdoi', 'doi', 'metadata.title', 'stats.views', '#versions', 'links.html', 'links.doi', 'created', 'updated']].sort_values(by=['stats.views'], ascending=False)\n",
    "output_verified.to_csv('./output_datasets_from_verified_dblp_authors.csv', header=True, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842fe55b-a85f-47c7-8acb-876898efb9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"In the past  10.000 unique hits there where\")\n",
    "print(f\"\\t * {count_unique_concept_doi} unique concept dois\")\n",
    "print()\n",
    "print(\"Number of Creators\")\n",
    "print(f\"\\t * min(#creators) = {creators_count_min}\")\n",
    "print(f\"\\t * max(#creators) = {creators_count_max}\")\n",
    "print(f\"\\t * median(#creators) = {creators_count_median}\")\n",
    "print(f\"\\t * mean(#creators) = {creators_count_mean}\")\n",
    "print(f\"\\t * quantile(0.95, #creators) = {creators_count_quantile95}\")\n",
    "print(\"Number of Versions\")\n",
    "print(f\"\\t * min(#versions) = {version_count_min}\")\n",
    "print(f\"\\t * max(#versions) = {version_count_max}\")\n",
    "print(f\"\\t * median(#versions) = {version_count_median}\")\n",
    "print(f\"\\t * mean(#versions) = {version_count_mean}\")\n",
    "print(f\"\\t * quantile(0.95, #versions) = {version_count_quantile95}\")\n",
    "print(f\"\\t * count(#versions == 1) = {version_count_exactly_one_version}\")\n",
    "print(f\"Number of historic Versions {(unique_historic_records['created'].min(), unique_historic_records['created'].max(), len(unique_historic_records))}\")\n",
    "print(f\"\\t * min(#versions) = {version_historic_count_min}\")\n",
    "print(f\"\\t * max(#versions) = {version_historic_count_max}\")\n",
    "print(f\"\\t * median(#versions) = {version_historic_count_median}\")\n",
    "print(f\"\\t * mean(#versions) = {version_historic_count_mean}\")\n",
    "print(f\"\\t * quantile(0.95, #versions) = {version_historic_count_quantile95}\")\n",
    "print(\"Communities\")\n",
    "print(f\"\\t * min(size(communities)) = {communities_min}\")\n",
    "print(f\"\\t * max(size(communities)) = {communities_max}\")\n",
    "print(f\"\\t * median(size(communities)) = {communities_median}\")\n",
    "print(f\"\\t * mean(size(communities)) = {communities_mean}\")\n",
    "print(f\"\\t * quantile(0.95, size(communities)) = {communities_quantile}\")\n",
    "print(f\"\\t * Top 20 communities: {top_20_communities}\")\n",
    "print()\n",
    "print(\"Changing Versions\")\n",
    "print(f\"\\t * Of the last {len(non_unique_hits)} non  unique updated datasets, there are\")\n",
    "print(f\"\\t * {multiple_versions_concept_doi_count} datasets that have been updated at least once\")\n",
    "print(f\"\\t * of them {multiple_versions_changing_authors_count} changed their author count\")\n",
    "print()\n",
    "print(\"DBPL authors\")\n",
    "print(f\"\\t * Number of signatures in dataset = {count_signatures}\")\n",
    "print(f\"\\t * Number of signatures with orcid = {count_signatures_with_orcid}\")\n",
    "print(f\"\\t * Number of records with at least one orcid = {count_records_with_at_least_one_orcid}\")\n",
    "print(f\"\\t * Number of records with a least one dblp author = {count_records_with_a_dblp_match}\")\n",
    "print(f\"\\t * Number of records with a least one verified dblp author = {count_records_with_a_verified_dblp_match}\")\n",
    "print(f\"\\t * Number of verified dblp orcids = {count_verified_dblp_orcid}\")\n",
    "print(\"DBPL authors historic dataset\")\n",
    "print(f\"\\t * Number of signatures in dataset = {count_signatures_hisotric}\")\n",
    "print(f\"\\t * Number of signatures with orcid = {count_signatures_with_orcid_hisotric}\")\n",
    "print(f\"\\t * Number of records with at least one orcid = {count_records_with_at_least_one_orcid_hisotric}\")\n",
    "print(f\"\\t * Number of records with a least one dblp author = {count_records_with_a_dblp_match_hisotric}\")\n",
    "print(f\"\\t * Number of records with a least one verified dblp author = {count_records_with_a_verified_dblp_match_hisotric}\")\n",
    "print(\"Updates to concepts\")\n",
    "print(f\"\\t * MIN avg_time_between_updates_by_concept in days: {concept_min_update_time}\")\n",
    "print(f\"\\t * MAX avg_time_between_updates_by_concept in days: {concept_max_update_time}\")\n",
    "print(f\"\\t * MEAN avg_time_between_updates_by_concept in days: {concept_mean_update_time}\")\n",
    "print(f\"\\t * MEDIAN avg_time_between_updates_by_concept in days: {concept_median_update_time}\")\n",
    "print(f\"\\t * QUANTILE 20% avg_time_between_updates_by_concept in days: {concept_quantile}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
